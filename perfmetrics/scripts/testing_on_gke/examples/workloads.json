{
  "_comment": "_ in the starting of element name indicates comment.",
  "TestConfig": {
    "workloadConfig": {
      "_description": "workloadConfig has an optional field runOnSSD (default true if missing), and an array of workloads.",
      "runOnSSD": true,
      "workloads": [
        {
          "_description": "This is a dummy fio workload (missing the 'fioWorkload' field), purely standing as a header and does not execute any workload. For it to execute a fio workload, it must have a valid 'fioWorkload' object and a valid 'bucket' attribute.",
          "_fioWorkload": {
            "_description": "Every fioWorkload must have fileSize, filesPerThread, numThreads, and blockSize fields. readTypes is an array of string values 'read' and 'randread'. If readTypes is missing, then it defaults to [\"read\",\"randread\"].",
            "fileSize": "64k",
            "filesPerThread": 20000,
            "numThreads": 50,
            "blockSize": "64K",
            "readTypes": ["read","randread"]
          },
          "bucket":"The bucket must have objects with name Workload.{i}/{j} for every i,j where i:0-{numThreads}-1, j:0-{filesPerThread}-1, and each of these objects must be of size {fileSize}. The buckets gke-* are all in us-central1, are owned by GKE team and are in their GCP project(s)."
        },
        {
          "fioWorkload": {
            "fileSize": "64k",
            "filesPerThread": 20000,
            "numThreads": 50,
            "blockSize": "64K",
            "readTypes": ["read"]
          },
          "bucket":"fio-64k-1m-us-west1",
          "_bucket_alt2":"fio-64k-1m-us-central1",
          "_bucket_alt3":"gke-fio-64k-1m"
        },
        {
          "fioWorkload": {
            "fileSize": "128K",
            "filesPerThread": 20000,
            "numThreads": 50,
            "blockSize": "128K",
            "readTypes": ["read"]
          },
          "bucket":"fio-128k-1m-us-west1",
          "_bucket_alt2":"fio-128k-1m-us-central1",
          "_bucket_alt3":"gke-fio-128k-1m"
        },
        {
          "fioWorkload": {
            "fileSize": "1M",
            "filesPerThread": 20000,
            "numThreads": 50,
            "blockSize": "256K",
            "readTypes": ["read","randread"]
          },
          "bucket":"fio-1mb-1m-us-west1",
          "_bucket_alt2":"fio-1mb-1m-us-central1",
          "_bucket_alt3":"gke-fio-1mb-1m"
        },
        {
          "fioWorkload": {
            "fileSize": "100M",
            "filesPerThread": 1000,
            "numThreads": 50,
            "blockSize": "1M"
          },
          "bucket":"fio-100mb-50k-us-west1",
          "_bucket_alt2":"fio-100mb-50k-us-central1",
          "_bucket_alt3":"gke-fio-100mb-50k"
        },
        {
          "fioWorkload": {
            "_description": "This workload's job file is configured differently from the rest. It has one file, whis is read in parallel depending on the value of numThreads (only 100 supported right now).",
            "fileSize": "200G",
            "filesPerThread": 1,
            "numThreads": 100,
            "blockSize": "1M"
          },
          "bucket":"fio-200gb-1-us-west1",
          "_bucket_alt2":"fio-200gb-1-us-central1",
          "_bucket_alt3":"gke-fio-200gb-1"
        },
        {
          "_description": "This is a dummy dlio workload (missing the 'dlioWorkload' field), purely standing as a header and does not execute any workload. For it to execute a dlio workload, it must have a valid 'dlioWorkload' object and a valid 'bucket' attribute.",
          "_dlioWorkload": {
            "_description": "Every dlioWorkload must have numFilesTrain, recordLength, and batchSizes fields. batchSizes is an array of integer values",
            "numFilesTrain": 500000,
            "recordLength": 102400,
            "batchSizes": [800,128]
          },
          "bucket":"The bucket must have objects with name 'train/', 'valid/', and train/img_{i}_of_{numFilesTrain}.npz for every i where i:0-{numFilesTrain}-1 and each train/img_{i}_of_{numFilesTrain}.npz must be of size {recordLength} bytes. The buckets gke-* are all in us-central1, are owned by GKE team and are in their GCP project(s)."
        },
        {
          "dlioWorkload": {
            "numFilesTrain": 500000,
            "recordLength": 102400,
            "batchSizes": [800,128]
          },
          "bucket":"dlio-unet3d-100kb-500k-us-west1",
          "_bucket_alt2":"dlio-unet3d-100kb-500k-us-central1",
          "_bucket_alt3":"gke-dlio-unet3d-100kb-500k"
        },
        {
          "dlioWorkload": {
            "numFilesTrain": 1000000,
            "recordLength": 512000,
            "batchSizes": [800,128]
          },
          "bucket":"dlio-unet3d-500kb-1m-us-west1",
          "_bucket_alt2":"dlio-unet3d-500kb-1m-us-central1",
          "_bucket_alt3":"gke-dlio-unet3d-500kb-1m"
        },
        {
          "dlioWorkload": {
            "numFilesTrain": 100000,
            "recordLength": 3145728,
            "batchSizes": [200]
          },
          "bucket":"dlio-unet3d-3mb-100k-us-west1",
          "_bucket_alt2":"dlio-unet3d-3mb-100k-us-central1",
          "_bucket_alt3":"gke-dlio-unet3d-3mb-100k"
        },
        {
          "dlioWorkload": {
            "numFilesTrain": 5000,
            "recordLength": 157286400,
            "batchSizes": [4]
          },
          "bucket":"dlio-unet3d-150mb-5k-us-west1",
          "_bucket_alt2":"dlio-unet3d-150mb-5k-us-central1",
          "_bucket_alt3":"gke-dlio-unet3d-150mb-5k"
        }
      ]
    }
  }
}
