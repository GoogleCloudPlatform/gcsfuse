# Copyright 2018 The Kubernetes Authors.
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: Pod
metadata:
  name: dlio-tester-{{ .Values.dlio.numFilesTrain }}-{{ .Values.dlio.recordLength }}-{{ .Values.dlio.batchSize }}-{{ .Values.scenario }}
  {{- if ne .Values.scenario "local-ssd" }}
  annotations:
    gke-gcsfuse/volumes: "true"
    gke-gcsfuse/memory-limit: "6Gi"
  {{- end }}
spec:
  restartPolicy: Never
  nodeSelector:
    cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
    node.kubernetes.io/instance-type: {{ .Values.nodeType }}
  containers:
  - name: dlio-tester
    image: {{ .Values.image }}
    resources:
      limits:
        cpu: {{ .Values.resourceLimits.cpu }}
        memory: {{ .Values.resourceLimits.memory }}
      requests:
        cpu: {{ .Values.resourceRequests.cpu }}
        memory: {{ .Values.resourceRequests.memory }}
    env:
      - name: RDMAV_FORK_SAFE
        value: "1"
    command:
      - "/bin/sh"
      - "-c"
      - |
        # Change the source code of dlio benchmark so that page cache is cleared at every epoch
        main_file="dlio_benchmark/main.py"
        x=$(grep -n "for epoch in range(1, self.epochs + 1):" $main_file | cut -f1 -d ':')
        x=$((x + 1))
        sed -i "${x} i \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ os.system(\"sudo sh -c 'echo 3 > /proc/sys/vm/drop_caches'\")" $main_file

        echo "Installing gsutil..."
        apt-get update && apt-get install -y apt-transport-https ca-certificates gnupg curl
        curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg
        echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
        apt-get update && apt-get install -y google-cloud-cli

        {{ if eq .Values.scenario "local-ssd" }}
        echo "Generating data for on Local SSD..."
        mkdir -p /data

        mpirun -np 20 dlio_benchmark workload=unet3d_a100 \
        ++workload.workflow.generate_data=True \
        ++workload.workflow.train=False \
        ++workload.dataset.data_folder=/data \
        ++workload.dataset.num_files_train={{ .Values.dlio.numFilesTrain }} \
        ++workload.dataset.record_length={{ .Values.dlio.recordLength }} \
        ++workload.dataset.record_length_stdev=0 \
        ++workload.dataset.record_length_resize=0

        echo "Sleeping 5 minutes to wait for Local SSD RAID to populate data."
        sleep 300
        {{ end }}

        echo "Testing {{ .Values.scenario }}"
        mpirun -np 8 dlio_benchmark workload=unet3d_a100 \
        ++workload.train.epochs=4 \
        ++workload.workflow.profiling=True \
        ++workload.profiling.profiler=iostat \
        ++workload.profiling.iostat_devices=[md0] \
        ++workload.dataset.data_folder=/data \
        ++workload.dataset.num_files_train={{ .Values.dlio.numFilesTrain }} \
        ++workload.reader.batch_size={{ .Values.dlio.batchSize }} \
        ++workload.dataset.record_length={{ .Values.dlio.recordLength }} \
        ++workload.reader.read_threads={{ .Values.dlio.readThreads }} \
        ++workload.output.folder=/logs/{{ .Values.dlio.numFilesTrain }}-{{ .Values.dlio.recordLength }}-{{ .Values.dlio.batchSize }}/{{ .Values.scenario }}

        {{ if eq .Values.scenario "gcsfuse-generic"}}
        echo "{{ .Values.gcsfuse.mountOptions }}" > /logs/{{ .Values.dlio.numFilesTrain }}-{{ .Values.dlio.recordLength }}-{{ .Values.dlio.batchSize }}/{{ .Values.scenario }}/gcsfuse_mount_options
        {{ end }}

        gsutil -m cp -R /logs gs://{{ .Values.bucketName }}/logs/$(date +"%Y-%m-%d-%H-%M")
    volumeMounts:
    - name: dshm
      mountPath: /dev/shm
    - name: logging-vol
      mountPath: /logs
    - name: data-vol
      mountPath: /data
  volumes:
  - name: dshm
    emptyDir:
      medium: Memory
  - name: logging-vol
    emptyDir: {}
  - name: data-vol
  {{- if eq .Values.scenario "local-ssd" }}
    emptyDir: {}
  {{- else if eq .Values.scenario "gcsfuse-generic" }}
    csi:
      driver: gcsfuse.csi.storage.gke.io
      volumeAttributes:
        bucketName: {{ .Values.bucketName }}
        gcsfuseLoggingSeverity: "info"
        mountOptions: "{{ .Values.gcsfuse.mountOptions }}"
  {{- else if eq .Values.scenario "gcsfuse-file-cache" }}
    csi:
      driver: gcsfuse.csi.storage.gke.io
      volumeAttributes:
        bucketName: {{ .Values.bucketName }}
        metadataCacheTTLSeconds: "{{ .Values.gcsfuse.metadataCacheTTLSeconds }}"
        metadataTypeCacheCapacity: "{{ .Values.gcsfuse.metadataTypeCacheCapacity }}"
        metadataStatCacheCapacity: "{{ .Values.gcsfuse.metadataStatCacheCapacity }}"
        fileCacheCapacity: "{{ .Values.gcsfuse.fileCacheCapacity }}"
        fileCacheForRangeRead: "{{ .Values.gcsfuse.fileCacheForRangeRead }}"
        gcsfuseLoggingSeverity: "debug"
        mountOptions: implicit-dirs
  {{- else }}
    csi:
      driver: gcsfuse.csi.storage.gke.io
      volumeAttributes:
        bucketName: {{ .Values.bucketName }}
        metadataCacheTTLSeconds: "{{ .Values.gcsfuse.metadataCacheTTLSeconds }}"
        metadataTypeCacheCapacity: "{{ .Values.gcsfuse.metadataTypeCacheCapacity }}"
        metadataStatCacheCapacity: "{{ .Values.gcsfuse.metadataStatCacheCapacity }}"
        gcsfuseLoggingSeverity: "debug"
        mountOptions: implicit-dirs
  {{- end }}
