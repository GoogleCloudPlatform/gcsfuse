# Python load testing tool

Python load testing tool is to generate load on the machine it is run, observe 
the usage of machine's resources (CPU and network) and then report those usages. 
The load is generated by executing a piece of python code in multiple threads 
in multiple processes. Users can define their own tasks, pass them to the tool 
and pass different flags to configure the load testing.

Example usage:
```
python3 load_test.py --task-file-path /path/to/task/module.py --num-processes 32 
--num-threads 2 --output-dir /dir/for/test/output --run-time 60
```
In the above example usage, two threads are spawned in each of the 32 processes
where each thread runs the task defined in /path/to/task/module.py in a
continuous loop till 60 seconds. After 60 seconds, the results containing 
resources' usages (CPU and network) and latencies are saved in
/dir/for/test/output.

# Prerequisites

* python3
* python packages mentioned in [requirements.txt]

[requirements.txt]: ./requirements.txt

# Supported flags
[load_test.py] is the script used that can be used to run load test. The script
accepts many flags to customize load testing configuration. Below are some 
important flags that can be passed to the script:
* ```--task-file-path```: Path to python module (file) containing task classes 
implementing task.LoadTestTask.
* ```--num-processes```: Number of processes to spawn in load tests with
  --num-threads threads where each thread runs the task.
* ```--num-threads```: Number of threads to run in each process spawned for 
load test. Each thread runs the task in a loop depending and terminate 
depending upon other flags.
* ```--run-time```: Duration in seconds for which to run the load test.
* ```--output-dir```: Path to directory where you want to save the output of 
load tests. One directory is created inside --output-dir for each task in 
--task-file-path file. If --only-print is also passed, then results are not 
saved.

For more details on the supported flags, their default values and uses, please
run load_test.py script with ```--help``` flag.

[load_test.py]: ./load_test.py

# Output metrics
The output of load test contains the following metrics:
* CPU metrics: Average CPU usage (%), Peak CPU usage (%) & variation of same 
over span of load test.
* Network Bandwidth: Average network upload bandwidth (%), 
Average network download bandwidth (%) & variation of same over span of load
test.
* Latencies: Min, mean, max latencies and 25th, 50th, 75th and 95th percentiles 
of latencies of task performed over span of load test.

The output of load test performed using task with name SampleTask is saved
under the directory ```output-dir/SampleTask```. The output contains:
* CPU usage variation chart image: An image of chart showing the variation in
CPU usage (%) over the span of load test. File name: cpu_variation.png
* Network bandwidth variation chart image: An image of chart showing the
variation in network bandwidth (MiB/sec) over the span of load test. File name:
net_bandwidth_variation.png
* Metrics JSON: A JSON file containing dictionary of all the metrics discussed
above. File name: metrics.json
* Comparison csv: A csv file containing the average CPU and network metrics for
all the tasks for comparison. File name: comparison.csv

# How to run

## Any task
Let's say we want to run a CPU intensive task parallely with 40 processes for 5
minutes (300s) and save the result in directory: ```~/output/CPU_TASK/```
* Make sure the prerequisites are installed.
* Set ```PYTHONPATH = gcsfuse/perfmetrics/scripts/load_tests/python/```
* Create a module for task class and implement LoadTestTask class in it i.e.
define task method. E.g.
```
from load_generator import task

class CPUTask(task.LoadTestTask):
  
  TASK_NAME = 'CPU_TASK'
  
  def task(self, assigned_process_id, assigned_thread_id):
    s = 0
    for i in range(1000000):
      s = s + assigned_process_id * assigned_thread_id
    return s
```
cpu_task.py
* Run the following command:
```
python3 load_test.py --task-file-path cpu_task.py --num-processes 40 
--output-dir ~/output --run-time 300
```
* The CPU usage (%) will be printed on terminal after the test is over and the
results will be saved in ```~/output/CPU_TASK/```.

## Predefined tasks
The following [tasks] are predefined in tasks directory:
* python_os.py: Tasks to read files from disk python's native open api. Can be 
used with GCSFuse if disk is mounted using GCSFuse.
* tf_gfile.py: Tasks to read files from GCS using tf's tf.io.gfile.Gfile api.
* tf_data.py: Tasks to read files from GCS using tf's tf.data api.

For more details on the tasks, please refer to the module level 
description of files.

Example usage:
```
python3 load_test.py --task-file-path python_os.py --num-processes 40 
--output-dir ~/output --run-time 300
```
Note: If performing GCSFuse tasks (in tf_data.py & python_os.py), make sure the
bucket is mounted using GCSFuse at /gcs directory.

[tasks]: tasks

# Miscellaneous
* GCSFuse has to be mounted for using with [python_os.py] tasks.
* It is recommended to keep --num-processes and --num-threads as 1 for 
[tf_data.py] tasks as the parallelism is inside those tasks.
* All the tasks defined under [tasks] directory are marked as read/write tasks.
So, load_test.py script tries to create files before running actual load tests.
* Task using tf apis require gcloud login on machine.

[python_os.py]: tasks/python_os.py
[tf_data.py]: tasks/tf_data.py
[tasks]: tasks
